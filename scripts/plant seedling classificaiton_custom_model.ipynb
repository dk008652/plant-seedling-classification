{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom glob import glob\npath = '../input/v2-plant-seedlings-dataset'\nclass_names = []\nclass_label = []\nimages = []\nclasses = []\n\nimage_per_class = {}\nfor class_folder in os.listdir(path):\n    \n    \n    class_folder_path = os.path.join(path, class_folder)\n    print(class_folder_path)\n    class_label = class_folder\n    class_names.append(class_folder)\n    image_per_class[class_label] = []\n    for images_path in glob(os.path.join(class_folder_path, \"*.png\")):\n        image_bgr = cv2.imread(images_path, cv2.IMREAD_COLOR)\n        image_per_class[class_label].append(image_bgr)\n        images.append(image_bgr)\n        classes.append(class_label)\n        \n        \n\n        \n        \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(image_per_class))\n\nfor key, values in image_per_class.items():\n    print(\"{0}--->{1}\".format(key, len(values)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = np.array(images)\nprint(type(images))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(len(class_names))\nprint(len(classes))\nprint(len(images))\n#size of image\nprint( (images[1].shape))\nprint(classes[1030:1036])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale = 256\ndef resize_image(img):\n    img = np.array(img).astype(np.uint8)\n    imag = cv2.resize(img, (scale,scale), interpolation = cv2.INTER_CUBIC)\n    #print(type(imag))\n    return imag\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# image resize making compatible for vgg16() model\nimages = [resize_image(img) for img in images]\nimages = np.array(images)\nprint(type(images))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#before giving images to model they are segmented for better result\n\n\n\ndef mask_image(image):\n    image = np.array(image).astype(np.uint8)\n    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    sensitivity = 35\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity, 255, 255])\n\n    mask = cv2.inRange(hsv_image, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    return mask\n\n\ndef segment_image(image):\n    mask = mask_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\n\n\ndef sharpen_image(image):\n    blurred_image = cv2.GaussianBlur(image, (0, 0), 3)\n    sharp_image = cv2.addWeighted(image, 1.5, blurred_image, -0.5, 0)\n    return sharp_image\n\n\ndef process_images(img):    \n    segmented_image = segment_image(img)\n    sharpened_image = sharpen_image(segmented_image)\n    return sharpened_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#processing all images for better result/accuracy\nprint(type(images))\nfor index, image in enumerate(images):\n    images[index] = process_images(image)\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import matplotlib.pyplot as plt\n#image = images[classes == \"Common Chickweed\"][2]\n#plt.imshow(image)\n#print(type(image))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mask_image_ = mask_image(image)\n#segmented_image = segment_image(image)\n#sharpened_image = sharpen_image(segmented_image)\n\n#fig, axs = plt.subplots(1, 4, figsize=(20, 20))\n#axs[0].imshow(image)\n#axs[1].imshow(mask_image_)\n#axs[2].imshow(segmented_image)\n#axs[3].imshow(sharpened_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = np.array(classes)\nfrom sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\ny = encoder.fit_transform(classes) #one hot encoded\n\n#print(y[0])\n#print(classes[0])\n#print(class_names)\n#print(len(class_names))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting dataset into train and test \nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(images, y, test_size = 0.2, random_state = 50)\n\nprint(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CNN Model\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalMaxPooling2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\n\n\n\nbatch_size = None\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation = 'relu',\n                 batch_input_shape = (batch_size, 256, 256, 3) ))\nmodel.add(Conv2D(filters = 64, kernel_size = (5, 5), padding = \"Same\", activation = 'relu') )\nmodel.add(MaxPool2D(pool_size = (2, 2)) )\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = \"Same\", activation = 'relu') )\nmodel.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = 'Same', activation = 'relu') )          \nmodel.add(MaxPool2D(pool_size=(2,2), strides = (2, 2)  ))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3, 3), padding = \"Same\", activation = 'relu') )\nmodel.add(Conv2D(filters = 128, kernel_size = (3, 3), padding = 'Same', activation = 'relu') )          \nmodel.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2) ) )\nmodel.add(Dropout(0.4))\n\nmodel.add(GlobalMaxPooling2D())\nmodel.add(Dense(256, activation = 'relu') )\nmodel.add(Dropout(0.5))\nmodel.add(Dense(12, activation = \"softmax\") )\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = Adam(lr = 0.001)\noptimizer = RMSprop(lr = 0.001, rho= 0.9, epsilon = 1e-08, decay = 0.0 )\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X_train))\nprint(len(Y_train))\nprint(len(X_test))\nprint(len(Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(len(Y_train[0]))\n#for i in range(4431):\n#    print(X_train[i].shape)\n\nX_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, test_size = 0.5, random_state = 20)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs = 10, validation_data = (X_val, Y_val), batch_size = batch_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model evaluation on validation data\nscore = model.evaluate(X_val, Y_val, verbose = 0.0, batch_size = batch_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Validation Loss: {}\".format(score[0]))\nprint(\"validdation accuracy: {}\".format(score[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data augmentation\n\naug = ImageDataGenerator(rotation_range = 10,\n                         zoom_range = 0.1,\n                         horizontal_flip = True,\n                         vertical_flip = False,\n                        width_shift_range = 0.1,\n                        height_shift_range = 0.1)\n#other parameters\n#zca_whitening = True\n#fill_mode = nearest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ReduceLROnPlateau\nlearning_rate_reduction = ReduceLROnPlateau(monitor = \"acc\",\n                                            patience =3,\n                                            verbose = 1.0,\n                                            factor = 0.5, \n                                           min_lr = 0.0001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_model = model.fit_generator(aug.flow(X_train, Y_train, batch_size = 38),\n                                    epochs = 10,\n                                    validation_data = (X_val,Y_val),\n                                    verbose = 2, \n                                    callbacks = [learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_aug = model.evaluate(X_val, Y_val, verbose = 2, batch_size = None, )\nprint(\"loss:{}\".format(score_aug[0]))\nprint(\"Accuracy:{}\".format(score_aug[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting loss and accuracy\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(2, 1, figsize = (15, 8))\nax[0].plot(trained_model.history[\"loss\"], color = \"r\", label = \"training_loss\")\nax[0].plot(trained_model.history[\"val_loss\"], color = \"b\", label = \"validation_loss\") \nlegend = ax[0].legend(loc = 'best', shadow = True)\n\nax[1].plot(trained_model.history[\"accuracy\"], color = \"r\", label = \"training_accuracy\")\nax[1].plot(trained_model.history[\"val_accuracy\"], color = \"b\", label = \"validation_accuracy\")\nlegend = ax[1].legend(loc = 'best', shadow = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_class = model.predict_classes(X_test)\nprint(type(predicted_class))\nprint(predicted_class.shape)\nwrong_prediction = X_test[predicted_class != np.argmax(Y_test)]\nprint(wrong_prediction.shape)\nprint(set(predicted_class))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=20\npred = model.predict_classes(np.array([wrong_prediction[i]]))[0]\nact = np.argmax(Y_test[i])\nprint(\"Predicted class: {}\".format(encoder.classes_[pred]))\nprint(\"Actual class: {}\".format(encoder.classes_[act]))\n\nplt.imshow(wrong_prediction[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}