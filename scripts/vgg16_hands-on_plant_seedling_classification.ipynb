{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import cv2\nfrom glob import glob\npath = '../input/v2-plant-seedlings-dataset'\nclass_names = []\nclass_label = []\nimages = []\nclasses = []\n\nimage_per_class = {}\nfor class_folder in os.listdir(path):\n    \n    \n    class_folder_path = os.path.join(path, class_folder)\n    print(class_folder_path)\n    class_label = class_folder\n    class_names.append(class_folder)\n    image_per_class[class_label] = []\n    for images_path in glob(os.path.join(class_folder_path, \"*.png\")):\n        image_bgr = cv2.imread(images_path, cv2.IMREAD_COLOR)\n        image_per_class[class_label].append(image_bgr)\n        images.append(image_bgr)\n        classes.append(class_label)\n        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(image_per_class))\n\nfor key, values in image_per_class.items():\n    print(\"{0}--->{1}\".format(key, len(values)))\n    \nimages = np.array(images)\nprint(type(images))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale = 256\ndef resize_image(img):\n    img = np.array(img).astype(np.uint8)\n    imag = cv2.resize(img, (scale,scale), interpolation = cv2.INTER_CUBIC)\n    #print(type(imag))\n    return imag\n\n# image resize making compatible for vgg16() model\nimages = [resize_image(img) for img in images]\nimages = np.array(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#before giving images to model they are segmented for better result\n\n\n\ndef mask_image(image):\n    image = np.array(image).astype(np.uint8)\n    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    sensitivity = 35\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity, 255, 255])\n\n    mask = cv2.inRange(hsv_image, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    return mask\n\n\ndef segment_image(image):\n    mask = mask_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\n\n\ndef sharpen_image(image):\n    blurred_image = cv2.GaussianBlur(image, (0, 0), 3)\n    sharp_image = cv2.addWeighted(image, 1.5, blurred_image, -0.5, 0)\n    return sharp_image\n\n\ndef process_images(img):    \n    segmented_image = segment_image(img)\n    sharpened_image = sharpen_image(segmented_image)\n    return sharpened_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#processing all images for better result/accuracy\nprint(type(images))\nfor index, image in enumerate(images):\n    images[index] = process_images(image)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = np.array(classes)\nfrom sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\ny = encoder.fit_transform(classes) #one hot encoded\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting dataset into train and test \nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(images, y, test_size = 0.2, random_state = 50)\n\nprint(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CNN Model\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalMaxPooling2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nimport keras\n\n# example of tending the vgg16 model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Input\n\nmodel_vgg16 = VGG16(include_top = False, weights = \"imagenet\", input_shape = (256, 256, 3))\nfor layer in model_vgg16.layers:\n\tlayer.trainable = False\n\noutput = model_vgg16.layers[-1].output\noutput = keras.layers.Flatten()(output)\n#output = keras.layers.Dense(32768, activation='relu')(output)\noutput = keras.layers.Dense(1024, activation='relu')(output)\noutput = Dense(12, activation='softmax')(output)\nmodel = Model(model_vgg16.input, output)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vgg_model.trainable = False\n#for layer in model.layers:\n    #layer.trainable = False\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = Adam(lr = 0.001)\noptimizer = RMSprop(lr = 0.001, rho= 0.9, epsilon = 1e-08, decay = 0.0 )\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nbatch_size = None\nX_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, test_size = 0.5, random_state = 20)\nmodel.fit(X_train, Y_train, epochs = 10, validation_data = (X_val, Y_val), batch_size = batch_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, Y_test, verbose = 0.0,  batch_size = batch_size)\nprint(\"Loss: \",score[0])\nprint(\"Accuracy:\", score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data augmentation\n\naug = ImageDataGenerator(rotation_range = 10,\n                         zoom_range = 0.1,\n                         horizontal_flip = True,\n                         vertical_flip = False,\n                        width_shift_range = 0.1,\n                        height_shift_range = 0.1)\n#other parameters\n#zca_whitening = True\n#fill_mode = nearest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ReduceLROnPlateau\nlearning_rate_reduction = ReduceLROnPlateau(monitor = \"acc\",\n                                            patience =3,\n                                            verbose = 1.0,\n                                            factor = 0.5, \n                                           min_lr = 0.0001)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_model = model.fit_generator(aug.flow(X_train, Y_train, batch_size = 38),\n                                    epochs = 10,\n                                    validation_data = (X_val,Y_val),\n                                    verbose = 2, \n                                    callbacks = [learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_aug = model.evaluate(X_test, Y_test, verbose = 2, batch_size = None, )\nprint(\"loss:{}\".format(score_aug[0]))\nprint(\"Accuracy:{}\".format(score_aug[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting loss and accuracy\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(2, 1, figsize = (15, 8))\nax[0].plot(trained_model.history[\"loss\"], color = \"r\", label = \"training_loss\")\nax[0].plot(trained_model.history[\"val_loss\"], color = \"b\", label = \"validation_loss\") \nlegend = ax[0].legend(loc = 'best', shadow = True)\n\nax[1].plot(trained_model.history[\"accuracy\"], color = \"r\", label = \"training_accuracy\")\nax[1].plot(trained_model.history[\"val_accuracy\"], color = \"b\", label = \"validation_accuracy\")\nlegend = ax[1].legend(loc = 'best', shadow = True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}